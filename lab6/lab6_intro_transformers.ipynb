{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63993517",
   "metadata": {},
   "source": [
    "# B√†i 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdca4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbfc981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b7b21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√¢u g·ªëc: Hanoi is the [MASK] of Vietnam.\n",
      "D·ª± ƒëo√°n: 'capital' v·ªõi ƒë·ªô tin c·∫≠y: 0.9991\n",
      " -> C√¢u ho√†n ch·ªânh: hanoi is the capital of vietnam.\n",
      "D·ª± ƒëo√°n: 'center' v·ªõi ƒë·ªô tin c·∫≠y: 0.0001\n",
      " -> C√¢u ho√†n ch·ªânh: hanoi is the center of vietnam.\n",
      "D·ª± ƒëo√°n: 'birthplace' v·ªõi ƒë·ªô tin c·∫≠y: 0.0001\n",
      " -> C√¢u ho√†n ch·ªânh: hanoi is the birthplace of vietnam.\n",
      "D·ª± ƒëo√°n: 'headquarters' v·ªõi ƒë·ªô tin c·∫≠y: 0.0001\n",
      " -> C√¢u ho√†n ch·ªânh: hanoi is the headquarters of vietnam.\n",
      "D·ª± ƒëo√°n: 'city' v·ªõi ƒë·ªô tin c·∫≠y: 0.0001\n",
      " -> C√¢u ho√†n ch·ªânh: hanoi is the city of vietnam.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\", framework=\"pt\")\n",
    "input_sentence = 'Hanoi is the [MASK] of Vietnam.'\n",
    "predictions = mask_filler(input_sentence, top_k=5)\n",
    "print(f'C√¢u g·ªëc: {input_sentence}')\n",
    "for pred in predictions:\n",
    "    print(f\"D·ª± ƒëo√°n: '{pred['token_str']}' v·ªõi ƒë·ªô tin c·∫≠y: {pred['score']:.4f}\")\n",
    "    print(f\" -> C√¢u ho√†n ch·ªânh: {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139052a",
   "metadata": {},
   "source": [
    "#### C√¢u h·ªèi:\n",
    "1. M√¥ h√¨nh ƒë√£ d·ª± ƒëo√°n ƒë√∫ng t·ª´ capital kh√¥ng?\n",
    "2. T·∫°i sao c√°c m√¥ h√¨nh Encoder-only nh∆∞ BERT l·∫°i ph√π h·ª£p cho t√°c v·ª• n√†y?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215beb6",
   "metadata": {},
   "source": [
    "#### Tr·∫£ l·ªùi:\n",
    "\n",
    "1. C√≥. V·ªõi c√¢u \"Hanoi is the [MASK] of Vietnam.\" th√¨ t·ª´ \"capital\" l√† ƒë√°p √°n ng·ªØ nghƒ©a ch√≠nh x√°c, n√™n c√≥ th·ªÉ coi l√† d·ª± ƒëo√°n ƒë√∫ng.\n",
    "\n",
    "2. C√≥ 2 √Ω ch√≠nh:\n",
    "\n",
    "- BERT l√† m√¥ h√¨nh hai chi·ªÅu (bidirectional): t·∫°i v·ªã tr√≠ [MASK] n√≥ nh√¨n ƒë∆∞·ª£c c·∫£ b√™n tr√°i l·∫´n b√™n ph·∫£i, n√™n ƒëo√°n t·ª´ b·ªã che r·∫•t t·ªët.\n",
    "\n",
    "- BERT ƒë∆∞·ª£c pre-train ƒë√∫ng b·∫±ng nhi·ªám v·ª• Masked Language Modeling (ƒëo√°n t·ª´ b·ªã che), n√™n b·∫£n ch·∫•t n√≥ sinh ra ƒë·ªÉ l√†m ƒë√∫ng ki·ªÉu task n√†y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cffe2ef",
   "metadata": {},
   "source": [
    "# B√†i 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a428ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√¢u m·ªìi: 'The best thing about learning NLP is'\n",
      "VƒÉn b·∫£n ƒë∆∞·ª£c sinh ra:\n",
      "The best thing about learning NLP is learning an interesting approach, and that's something I am thankful for the NLP community. Having so many of my own students working with these students are invaluable. Also, because I'm such a beginner, I\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", framework=\"pt\")\n",
    "prompt = \"The best thing about learning NLP is\"\n",
    "generated_texts = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(f\"C√¢u m·ªìi: '{prompt}'\")\n",
    "for text in generated_texts:\n",
    "    print(\"VƒÉn b·∫£n ƒë∆∞·ª£c sinh ra:\")\n",
    "    print(text['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c15f99",
   "metadata": {},
   "source": [
    "#### C√¢u h·ªèi:\n",
    "\n",
    "1. K·∫øt qu·∫£ sinh ra c√≥ h·ª£p l√Ω kh√¥ng?\n",
    "2. T·∫°i sao c√°c m√¥ h√¨nh Decoder-only nh∆∞ GPT l·∫°i ph√π h·ª£p cho t√°c v·ª• n√†y?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2c17c",
   "metadata": {},
   "source": [
    "#### Tr·∫£ l·ªùi:\n",
    "\n",
    "1. T∆∞∆°ng ƒë·ªëi h·ª£p l√Ω.\n",
    "\n",
    "- V·ªÅ ng·ªØ nghƒ©a & ch·ªß ƒë·ªÅ:\n",
    "C√¢u m·ªìi l√† \"The best thing about learning NLP is\", model sinh ti·∫øp:\n",
    "\n",
    "\"learning an interesting approach, and that's something I am thankful for the NLP community. Having so many of my own students working with these students are invaluable. Also, because I'm such a beginner, I ...\"\n",
    "\n",
    "=> N·ªôi dung v·∫´n xoay quanh h·ªçc NLP, c·ªông ƒë·ªìng NLP, sinh vi√™n / h·ªçc tr√≤, ƒë√∫ng ch·ªß ƒë·ªÅ, kh√¥ng b·ªã l·∫°c sang chuy·ªán kh√°c.\n",
    "\n",
    "- V·ªÅ ng√¥n ng·ªØ:\n",
    "\n",
    "C√¢u vƒÉn kh√° t·ª± nhi√™n, ki·ªÉu ng∆∞·ªùi th·∫≠t n√≥i, kh√¥ng ph·∫£i chu·ªói t·ª´ v√¥ nghƒ©a.\n",
    "C√≥ v√†i ch·ªó h∆°i l·ªßng c·ªßng / l·∫∑p (‚Äúmy own students working with these students‚Äù), v√† b·ªã d·ª´ng gi·ªØa ch·ª´ng ·ªü ch·ªØ ‚ÄúI‚Äù.\n",
    "\n",
    "=> H·ª£p l√Ω v·ªÅ √Ω nghƒ©a v√† ch·ªß ƒë·ªÅ, nh∆∞ng ch∆∞a ho√†n h·∫£o: h∆°i l·∫∑p v√† b·ªã c·∫Øt ngang do chi·ªÅu d√†i sinh gi·ªõi h·∫°n.\n",
    "\n",
    "2. GPT ƒë∆∞·ª£c train ƒë·ªÉ ƒëo√°n token ti·∫øp theo ‚Üí ƒë√∫ng y b√†i to√°n ‚Äúcho c√¢u m·ªìi, sinh ti·∫øp‚Äù.\n",
    "\n",
    "Ki·∫øn tr√∫c decoder-only + causal attention m√¥ ph·ªèng ƒë√∫ng c√°ch ta vi·∫øt t·ª´ tr√°i sang ph·∫£i, n√™n r·∫•t h·ª£p ƒë·ªÉ sinh ƒëo·∫°n vƒÉn ti·∫øp n·ªëi m·∫°ch l·∫°c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c3b6f",
   "metadata": {},
   "source": [
    "# B√†i 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee12b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector bi·ªÉu di·ªÖn c·ªßa c√¢u:\n",
      "tensor([[-6.3874e-02, -4.2837e-01, -6.6779e-02, -3.8430e-01, -6.5785e-02,\n",
      "         -2.1826e-01,  4.7636e-01,  4.8659e-01,  4.0136e-05, -7.4273e-02,\n",
      "         -7.4741e-02, -4.7635e-01, -1.9773e-01,  2.4824e-01, -1.2162e-01,\n",
      "          1.6678e-01,  2.1045e-01, -1.4576e-01,  1.2636e-01,  1.8635e-02,\n",
      "          2.4640e-01,  5.7090e-01, -4.7014e-01,  1.3782e-01,  7.3650e-01,\n",
      "         -3.3808e-01, -5.0330e-02, -1.6452e-01, -4.3517e-01, -1.2900e-01,\n",
      "          1.6516e-01,  3.4004e-01, -1.4930e-01,  2.2422e-02, -1.0488e-01,\n",
      "         -5.1916e-01,  3.2964e-01, -2.2162e-01, -3.4206e-01,  1.1993e-01,\n",
      "         -7.0148e-01, -2.3126e-01,  1.1224e-01,  1.2550e-01, -2.5191e-01,\n",
      "         -4.6374e-01, -2.7261e-02, -2.8415e-01, -9.9249e-02, -3.7018e-02,\n",
      "         -8.9192e-01,  2.5005e-01,  1.5816e-01,  2.2701e-01, -2.8497e-01,\n",
      "          4.5300e-01,  5.0929e-03, -7.9441e-01, -3.1007e-01, -1.7403e-01,\n",
      "          4.3029e-01,  1.6816e-01,  1.0590e-01, -4.8987e-01,  3.1856e-01,\n",
      "          3.2861e-01, -1.3403e-02,  1.8807e-01, -1.0905e+00,  2.1009e-01,\n",
      "         -6.7579e-01, -5.7076e-01,  8.5946e-02,  1.9121e-01, -3.3818e-01,\n",
      "          2.7744e-01, -4.0539e-01,  3.1305e-01, -4.1197e-01, -5.6820e-01,\n",
      "         -3.9074e-01,  4.0747e-01,  9.9898e-02,  2.3719e-01,  1.0154e-01,\n",
      "         -2.5670e-01, -2.0583e-01,  1.1762e-01, -5.1439e-01,  4.0979e-01,\n",
      "          1.2149e-01,  1.9333e-02, -5.9029e-02, -2.0141e-01,  7.0860e-01,\n",
      "         -6.4609e-02,  2.4780e-02, -9.0586e-03,  1.9667e-02,  3.0815e-01,\n",
      "         -4.9832e-02, -1.0691e+00,  6.1072e-01, -4.9722e-02, -1.5156e-01,\n",
      "         -6.7778e-02,  4.7812e-02,  5.2103e-01,  1.6951e-01,  1.0146e-02,\n",
      "          5.3093e-01, -7.8189e-02,  6.5843e-02, -2.9383e-01, -4.6045e-01,\n",
      "          4.2071e-01,  1.1822e-01,  2.3631e-01, -4.5379e-02, -1.3740e-01,\n",
      "         -4.4018e-01, -6.8122e-02,  1.9935e-01,  8.7062e-01, -2.2603e-01,\n",
      "          3.3604e-01,  2.0236e-01,  3.7898e-01,  1.9533e-01, -3.0366e-01,\n",
      "          3.8633e-01,  6.1949e-01,  6.8663e-01, -1.8968e-01, -3.6815e-01,\n",
      "         -1.6616e-01, -7.0828e-02, -3.4610e-01, -8.5325e-01,  4.6645e-02,\n",
      "          2.8512e-01,  1.0890e-01,  2.5938e-01, -4.2975e-01,  4.3345e-01,\n",
      "          2.0637e-01, -3.8656e-01, -3.8187e-02,  3.6925e-01,  3.0130e-01,\n",
      "          4.0251e-01,  1.2887e-01, -3.7689e-01, -3.4447e-01, -4.2116e-01,\n",
      "         -1.0252e-01, -8.9736e-02,  4.7384e-01,  8.1716e-02,  1.5885e-01,\n",
      "          7.6674e-01,  3.4493e-01,  9.8548e-04,  4.8932e-02,  2.6132e-01,\n",
      "          3.8329e-02, -2.0036e-01,  2.6654e-01,  9.3773e-02, -4.6779e-02,\n",
      "         -4.0519e-01, -4.4310e-01,  6.1268e-01, -1.8950e-01, -3.8333e-01,\n",
      "          2.0583e-01,  1.5379e-01, -1.4664e-01,  5.3847e-01, -3.9618e-01,\n",
      "         -2.0599e+00,  6.7052e-01,  2.1112e-01, -4.7306e-01,  3.4865e-01,\n",
      "         -2.9919e-01,  5.4614e-01, -5.3924e-01, -2.4877e-01, -2.9070e-02,\n",
      "         -2.0319e-01, -7.3276e-02, -3.8147e-01, -5.4455e-01,  3.5050e-01,\n",
      "         -1.1249e-01, -2.1471e-01, -3.8439e-01, -1.0760e-01, -8.8821e-02,\n",
      "          2.5263e-01,  2.1448e-01,  5.5799e-02, -6.5411e-02,  9.9837e-02,\n",
      "          3.3435e-01,  2.4018e-01,  2.9875e-02, -1.1191e-01,  5.4330e-01,\n",
      "         -5.5214e-01,  1.1125e+00,  5.4141e-01, -7.4160e-02,  3.5337e-01,\n",
      "          1.2313e-01,  3.4856e-02, -2.8568e-01, -1.2517e-01, -4.4332e-02,\n",
      "          1.3323e-01, -2.4995e-01, -4.9833e-01,  4.1959e-01, -3.1580e-01,\n",
      "          6.1942e-01,  3.1113e-01,  4.8846e-01,  6.1518e-01, -3.6326e-02,\n",
      "          2.1294e-02, -3.5715e-01,  5.9126e-01,  1.5102e-01, -2.9641e-01,\n",
      "          2.9441e-01, -1.4139e-01,  1.1662e-01, -3.6223e-01, -1.4621e-01,\n",
      "          6.5255e-02,  3.9270e-01,  3.8543e-01, -2.3996e-01, -3.1482e-01,\n",
      "         -4.6860e-01, -1.1920e-01,  8.6235e-02, -3.4597e-02, -3.6275e-01,\n",
      "         -3.9838e-01, -3.6006e-01, -1.9672e-01, -2.7738e-01, -4.1097e-01,\n",
      "          3.6456e-01, -2.6012e-01,  1.2587e-01,  1.2752e-01,  5.4261e-01,\n",
      "          1.0569e-01,  3.5704e-01,  1.4766e-01,  4.4929e-01, -8.1255e-01,\n",
      "         -3.0409e-02,  5.8063e-02,  2.0699e-01,  6.6129e-01,  3.9243e-01,\n",
      "         -6.8644e-01, -8.3415e-01, -1.2654e-01,  1.9644e-01, -4.0900e-01,\n",
      "         -6.3776e-02, -1.8780e-01,  7.9474e-02, -1.7443e-01,  3.1936e-01,\n",
      "          3.6761e-01,  4.3044e-01, -1.7471e-01,  1.3718e-01,  1.4272e-01,\n",
      "         -6.0642e-01,  2.3549e-01,  2.7794e-01,  1.0539e-01, -4.5836e-01,\n",
      "         -3.2561e-01,  1.5293e-02, -2.7672e-01, -4.8611e-01,  3.9087e-01,\n",
      "          3.6016e-01,  6.3403e-01, -1.2816e-01, -1.6720e-02, -3.0123e-01,\n",
      "         -1.7321e-01, -6.7296e-01, -2.7015e-01, -1.2533e-01, -8.0565e-01,\n",
      "          3.6115e-01,  1.7370e-01, -3.5578e-01, -2.1725e+00, -2.8102e-02,\n",
      "         -2.6774e-02, -2.2444e-01,  3.1249e-02,  6.4419e-02, -1.5017e-01,\n",
      "         -3.4460e-01, -5.5676e-01,  1.8039e-01, -4.2200e-01, -9.1074e-01,\n",
      "         -3.1347e-03,  7.2439e-01,  3.9006e-01, -4.4129e-02, -4.4785e-02,\n",
      "          2.8708e-02, -1.2432e-01,  6.9166e-01, -1.3227e-02, -2.3540e-02,\n",
      "         -7.0616e-02, -4.5062e-01,  4.5705e-01,  3.3198e-01, -2.2727e-01,\n",
      "          3.2434e-01, -4.5709e-01, -5.1586e-01, -1.5693e-01, -1.0897e-01,\n",
      "          3.9317e-01, -2.5950e-01, -1.5326e-01,  3.3276e-01,  3.2522e-01,\n",
      "         -2.5241e-01,  4.7946e-01, -3.7339e-01, -2.8146e-01,  7.7629e-02,\n",
      "          2.7131e-01, -3.7212e-01,  6.1400e-01, -2.9269e-01, -4.4389e-01,\n",
      "         -3.7750e-01,  2.7135e-01,  3.6869e-01, -1.6904e-01, -1.7583e-01,\n",
      "          2.9626e-01,  2.9393e-01, -8.2030e-03,  3.4545e-02,  4.5846e-01,\n",
      "          3.0137e-01,  1.6171e-01, -2.7772e-01,  5.2397e-01, -6.1950e-01,\n",
      "         -2.4817e-02, -5.1943e-02,  3.6764e-01, -5.8404e-01, -2.6651e-01,\n",
      "         -7.5761e-02, -1.7428e-01,  4.1535e-01, -2.7556e-01, -5.6795e-02,\n",
      "         -4.3509e-01, -9.6659e-01, -1.1800e-01, -3.8004e-01,  2.7555e-01,\n",
      "         -2.9743e-01,  2.4023e-01, -3.8869e-01, -4.0248e-01, -8.3882e-01,\n",
      "         -1.0652e-01, -9.4192e-02,  1.4810e-01,  9.0848e-03,  1.4658e-01,\n",
      "         -1.4813e-01, -1.6078e-01, -4.3130e-01, -8.0684e-02,  4.3722e-01,\n",
      "          4.2623e-01,  3.3201e-01, -2.8283e-01,  2.0751e-01,  5.9093e-01,\n",
      "         -6.3454e-01,  5.7386e-01, -2.9870e-01,  1.0221e-02, -4.7624e-01,\n",
      "          4.9509e-01,  4.7470e-02,  1.3193e-01,  3.6281e-01, -1.1642e+00,\n",
      "          3.8372e-01,  1.7071e-01,  3.8881e-01,  1.7703e-01, -4.7019e-01,\n",
      "          1.2768e-01, -1.3409e-01, -2.8794e-01,  3.2066e-01, -3.7853e-01,\n",
      "          4.6259e-01,  5.2343e-01,  3.0741e-01,  2.7410e-01,  4.9933e-01,\n",
      "         -5.6466e-01, -3.4677e-01, -6.6572e-01, -1.3347e-01, -8.5910e-02,\n",
      "          6.2486e-02, -3.9922e-01, -3.5880e-01, -5.8337e-01, -1.3556e-02,\n",
      "         -1.6812e-01,  1.3949e-01,  2.9142e-01, -4.5623e-01, -1.0705e-01,\n",
      "          6.6569e-01,  7.6614e-01, -1.9306e-01,  4.3854e-01,  2.8110e-01,\n",
      "         -3.6835e-01, -1.6012e-01, -2.5005e-01,  7.6297e-01,  1.9653e-01,\n",
      "         -1.8120e-01,  1.1889e-03,  1.8755e-01, -1.8990e-01, -2.3725e-01,\n",
      "          3.2633e-02, -2.7723e-01, -4.7986e-02, -6.2333e-01,  2.6807e-01,\n",
      "         -1.2293e-01, -2.7098e-01, -6.9677e-01,  1.5738e-01,  5.3557e-01,\n",
      "          1.2760e-01, -1.7979e-02,  1.2769e-01, -5.6452e-02,  6.7965e-02,\n",
      "          1.8555e-01, -3.6374e-01,  2.8518e-01, -4.3920e-01, -2.4276e-01,\n",
      "          5.1755e-01, -2.3519e-01,  6.4010e-02,  3.9268e-01,  5.7986e-01,\n",
      "         -1.7500e-01,  7.1670e-02,  5.7915e-01,  5.1699e-02, -1.1081e-03,\n",
      "         -4.8444e-02,  1.5531e-01,  2.8402e-01,  6.8268e-01,  8.1524e-02,\n",
      "          1.5325e-01,  1.9466e-01,  1.2260e-02, -3.3223e-01,  2.5763e-02,\n",
      "         -1.6071e-01, -3.7663e-01, -7.3670e-01, -5.0067e-01,  1.1540e-01,\n",
      "         -3.3788e-01,  1.2889e-01,  2.1528e-02,  6.1149e-01,  3.3549e-01,\n",
      "         -2.0217e-01, -6.3961e-02,  2.4056e-02, -9.3071e-02, -2.7771e-02,\n",
      "          1.8373e-01, -4.1812e-02, -1.0456e-01, -2.7569e-01, -3.9216e-01,\n",
      "         -3.2092e-01, -1.0158e+00,  1.6407e-01,  4.5044e-02,  2.3079e-01,\n",
      "          2.6936e-02, -2.1047e-01, -3.1392e-01, -4.6154e-01, -4.0347e-01,\n",
      "          7.3271e-02,  1.1470e-01, -2.4129e-01, -3.6199e-01, -5.3254e-01,\n",
      "         -5.2185e-01, -4.0713e-01,  2.1619e-02,  1.4186e-01, -1.2105e-01,\n",
      "         -1.4055e-02, -4.2986e-02, -1.2459e-01, -6.6652e-01, -6.4169e-01,\n",
      "         -2.2399e-01,  6.2556e-02, -3.3324e-01,  1.8865e-02,  1.6465e-01,\n",
      "         -2.8729e-02, -5.9477e-01,  2.0963e-02, -3.3761e-01,  1.8089e-01,\n",
      "          7.4362e-01,  1.5554e-01,  2.7824e-01, -2.1975e-01,  5.1316e-01,\n",
      "         -3.9708e-01, -2.4769e-01,  4.3027e-01, -2.3078e-01, -2.9392e-01,\n",
      "          1.3250e-01, -6.1646e-01,  2.6501e-01,  5.6891e-01, -1.3585e-01,\n",
      "         -1.2774e-01,  8.1189e-01,  3.6497e-01,  5.0178e-01,  2.9736e-01,\n",
      "          8.7772e-01,  7.3390e-02,  2.5788e-01, -3.3609e-01,  8.8207e-02,\n",
      "          2.1283e-02,  1.4487e-01,  7.6685e-03, -3.9123e-01, -6.3919e-02,\n",
      "         -3.7236e-01,  8.2941e-02,  3.0821e-02,  3.1530e-02,  2.0262e-01,\n",
      "         -5.0066e-01, -1.2373e-01,  2.2661e-01,  1.6069e-01, -3.6415e-01,\n",
      "          2.3418e-01, -1.6900e-01, -1.3540e-01, -1.6677e-01,  1.5227e-01,\n",
      "         -2.6064e-01,  4.4844e-02, -3.4592e-02, -1.2043e-01,  6.4724e-01,\n",
      "          4.8944e-01, -3.0347e-01, -2.3118e-01, -8.3765e-02,  2.2163e-01,\n",
      "          1.0404e-01,  1.3495e-01, -5.3097e-01,  1.4525e-01,  4.9890e-01,\n",
      "         -4.9265e-01,  3.7358e-01,  2.2077e-01, -5.4248e-02, -6.7141e-02,\n",
      "          6.2195e-01,  4.6524e-01, -4.2303e-01, -3.2715e-01,  3.8370e-01,\n",
      "         -5.7111e-01, -1.6922e-01,  4.2353e-01, -2.0156e-01, -1.2482e-01,\n",
      "          4.3334e-01, -4.0269e-02, -5.8664e-01,  7.2658e-01, -5.5645e-01,\n",
      "         -5.7467e-02, -2.1052e-01,  1.0038e-01, -2.5420e-03,  7.7563e-01,\n",
      "         -3.9355e-01,  6.4184e-01, -5.9658e-01,  2.1974e-02,  1.8323e-01,\n",
      "          1.7593e-01,  4.8541e-01, -4.6241e-01,  3.5692e-01,  3.2622e-01,\n",
      "         -2.0756e-01,  5.7904e-01, -2.7194e-01, -5.2925e-01,  7.4888e-02,\n",
      "         -2.6069e-02,  3.5997e-01,  5.5750e-01,  3.2160e-01,  4.0078e-01,\n",
      "          5.1017e-01, -4.6596e-02,  2.9056e-01,  2.4928e-01,  2.0993e-01,\n",
      "          4.9611e-01, -4.1696e-02, -1.5711e-01,  1.5638e-01,  8.1301e-02,\n",
      "          3.2564e-01, -2.6684e-01, -2.1355e-01,  1.9676e-01,  4.6960e-01,\n",
      "          1.5972e-01, -2.5917e-01, -1.0547e-01,  1.3562e-01,  3.5989e-01,\n",
      "         -1.0882e-01, -7.1567e-02, -5.3039e-01,  8.8760e-01, -3.4283e-01,\n",
      "         -5.0051e-02, -4.8836e-01,  2.0944e-01,  2.6859e-01,  4.4361e-01,\n",
      "         -4.6622e-01, -1.3640e-01, -1.4363e-01, -3.5663e-01, -1.1210e-01,\n",
      "         -1.9890e-01, -1.2909e-01, -3.0794e-03, -6.2016e-02, -4.2345e-01,\n",
      "          2.7059e-01, -3.1317e-01,  5.7516e-01, -2.2517e-03,  1.7034e-01,\n",
      "          3.9410e-01,  8.1126e-01, -3.6260e-01,  5.2088e-01, -5.4591e-01,\n",
      "         -5.8637e-02,  1.5576e-01,  1.7441e-01,  1.3422e-01, -4.4369e-01,\n",
      "          2.6824e-01, -2.6424e-01, -5.6734e-01,  2.7223e-01,  5.5829e-01,\n",
      "         -9.1910e-01,  2.2039e-01, -3.5612e-01,  1.3164e-01, -1.1517e-01,\n",
      "         -2.0684e-01, -2.7872e-02,  3.9112e-01, -6.6897e-01, -3.8353e-01,\n",
      "         -5.6090e-02,  8.0477e-01, -2.5700e-01, -1.0725e-01,  7.5041e-02,\n",
      "          2.4736e-01, -6.1457e-01, -1.9508e-01,  5.4607e-01,  3.3887e-01,\n",
      "          2.7338e-01,  4.4597e-01,  4.4805e-01, -7.3450e-01,  2.2959e-01,\n",
      "         -3.8096e-02, -1.4963e-01, -2.4957e-01, -2.8457e-01,  5.6483e-01,\n",
      "          5.4733e-02,  8.0650e-02, -1.2184e+00,  5.7510e-01,  1.3625e-01,\n",
      "         -4.4055e-01,  6.9751e-02, -4.0260e-01,  1.0932e-01, -6.6830e-02,\n",
      "         -3.9554e-02, -5.4193e-01, -4.4191e-01,  2.4927e-01,  6.6517e-01,\n",
      "         -1.7534e-01, -1.2388e-01,  3.1970e-01]])\n",
      "K√≠ch th∆∞·ªõc c·ªßa vector: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "sentences = ['This is a sample sentence.']\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "attention_mask = inputs['attention_mask']\n",
    "mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "sentence_embedding = sum_embeddings / sum_mask\n",
    "print('Vector bi·ªÉu di·ªÖn c·ªßa c√¢u:')\n",
    "print(sentence_embedding)\n",
    "print('K√≠ch th∆∞·ªõc c·ªßa vector:', sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e362a9",
   "metadata": {},
   "source": [
    "#### C√¢u h·ªèi:\n",
    "\n",
    "1. K√≠ch th∆∞·ªõc (chi·ªÅu) c·ªßa vector bi·ªÉu di·ªÖn l√† bao nhi√™u? Con s·ªë n√†y t∆∞∆°ng ·ª©ng v·ªõi tham s·ªë n√†o c·ªßa m√¥ h√¨nh BERT?\n",
    "2. T·∫°i sao ch√∫ng ta c·∫ßn s·ª≠ d·ª•ng attention_mask khi th·ª±c hi·ªán Mean Pooling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070e615",
   "metadata": {},
   "source": [
    "# Tr·∫£ l·ªùi:\n",
    "\n",
    "1. K√≠ch th∆∞·ªõc: torch.Size([1, 768])\n",
    "\n",
    "- 1: batch size (1 c√¢u)\n",
    "- 768: chi·ªÅu c·ªßa hidden state\n",
    "- Con s·ªë 768 t∆∞∆°ng ·ª©ng v·ªõi tham s·ªë hidden_size c·ªßa m√¥ h√¨nh bert-base-uncased. ƒê√¢y l√† s·ªë chi·ªÅu c·ªßa vector ƒë·∫ßu ra t·ª´ m·ªói l·ªõp Transformer trong BERT. C√°c bi·∫øn th·ªÉ BERT kh√°c c√≥ hidden_size kh√°c nhau.\n",
    "\n",
    "2. attention_mask gi√∫p lo·∫°i b·ªè ·∫£nh h∆∞·ªüng c·ªßa c√°c token padding khi t√≠nh trung b√¨nh. Khi x·ª≠ l√Ω nhi·ªÅu c√¢u c√πng l√∫c (batch), c√°c c√¢u ng·∫Øn s·∫Ω ƒë∆∞·ª£c th√™m padding ƒë·ªÉ c√πng ƒë·ªô d√†i. N·∫øu kh√¥ng d√πng attention_mask, c√°c token padding s·∫Ω ƒë∆∞·ª£c t√≠nh v√†o trung b√¨nh, l√†m vector bi·ªÉu di·ªÖn kh√¥ng ch√≠nh x√°c. Attention_mask ƒë·∫£m b·∫£o ch·ªâ c√°c token th·ª±c s·ª± c·ªßa c√¢u ƒë∆∞·ª£c t√≠nh, cho k·∫øt qu·∫£ ƒë√∫ng ng·ªØ nghƒ©a."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
