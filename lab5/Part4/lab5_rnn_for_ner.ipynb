{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff5694a",
   "metadata": {},
   "source": [
    "## Task 1: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4f9fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87777dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CoNLL-2003 dataset from local files...\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Train size: 14041 sentences\n",
      "Validation size: 3250 sentences\n",
      "Test size: 3453 sentences\n",
      "\n",
      "Example sentence from training set:\n",
      "Tokens: BRUSSELS 1996-08-22...\n",
      "Tags: B-LOC O...\n"
     ]
    }
   ],
   "source": [
    "def load_conll_file(filepath):\n",
    "    \"\"\"\n",
    "    Load CoNLL-2003 format file\n",
    "    Format: token pos chunk ner_tag\n",
    "    Sentences are separated by blank lines\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line == \"\" or line.startswith(\"-DOCSTART-\"):\n",
    "                # End of sentence or document marker\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "                    current_sentence = []\n",
    "                    current_tags = []\n",
    "            else:\n",
    "                # Parse line: token pos chunk ner_tag\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:\n",
    "                    token = parts[0]\n",
    "                    ner_tag = parts[3]  # Last column is NER tag\n",
    "                    current_sentence.append(token)\n",
    "                    current_tags.append(ner_tag)\n",
    "        \n",
    "        # Add last sentence if exists\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            tags.append(current_tags)\n",
    "    \n",
    "    return sentences, tags\n",
    "\n",
    "# Load all splits\n",
    "print(\"Loading CoNLL-2003 dataset from local files...\")\n",
    "DATA_DIR = \"e:/NLP/Lab5/part4/data/conll2003_files\"\n",
    "\n",
    "train_sentences, train_tags = load_conll_file(f\"{DATA_DIR}/train.txt\")\n",
    "val_sentences, val_tags = load_conll_file(f\"{DATA_DIR}/valid.txt\")\n",
    "test_sentences, test_tags = load_conll_file(f\"{DATA_DIR}/test.txt\")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Train size: {len(train_sentences)} sentences\")\n",
    "print(f\"Validation size: {len(val_sentences)} sentences\")\n",
    "print(f\"Test size: {len(test_sentences)} sentences\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nExample sentence from training set:\")\n",
    "print(f\"Tokens: {' '.join(train_sentences[2][:10])}...\")\n",
    "print(f\"Tags: {' '.join(train_tags[2][:10])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9cea04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Tag Distribution in Training Set:\n",
      "  B-LOC            7140 (3.51%)\n",
      "  B-MISC           3438 (1.69%)\n",
      "  B-ORG            6321 (3.10%)\n",
      "  B-PER            6600 (3.24%)\n",
      "  I-LOC            1157 (0.57%)\n",
      "  I-MISC           1155 (0.57%)\n",
      "  I-ORG            3704 (1.82%)\n",
      "  I-PER            4528 (2.22%)\n",
      "  O              169578 (83.28%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze tag distribution\n",
    "all_tags = [tag for sent_tags in train_tags for tag in sent_tags]\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "print(\"\\nNER Tag Distribution in Training Set:\")\n",
    "for tag, count in sorted(tag_counts.items()):\n",
    "    print(f\"  {tag:<12} {count:>8} ({count/len(all_tags)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bafc923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary sizes:\n",
      "  Word vocabulary: 23625 words\n",
      "  Tag vocabulary: 9 tags\n",
      "\n",
      "Tag to index mapping (sorted):\n",
      "  0: O\n",
      "  1: B-LOC\n",
      "  2: B-MISC\n",
      "  3: B-ORG\n",
      "  4: B-PER\n",
      "  5: I-LOC\n",
      "  6: I-MISC\n",
      "  7: I-ORG\n",
      "  8: I-PER\n"
     ]
    }
   ],
   "source": [
    "# Build vocabularies\n",
    "def build_vocab(sentences, tags):\n",
    "    # Build word vocabulary\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        word_counts.update(sentence)\n",
    "    \n",
    "    # Create word_to_ix with special tokens\n",
    "    word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word in word_counts:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    \n",
    "    # Build tag vocabulary with consistent ordering\n",
    "    # Collect all unique tags first\n",
    "    all_tags = set()\n",
    "    for sent_tags in tags:\n",
    "        for tag in sent_tags:\n",
    "            all_tags.add(tag)\n",
    "    \n",
    "    # Sort tags for reproducibility (O first, then B-*, then I-* alphabetically)\n",
    "    sorted_tags = sorted(all_tags, key=lambda t: (0 if t == 'O' else (1 if t.startswith('B-') else 2), t))\n",
    "    tag_to_ix = {tag: i for i, tag in enumerate(sorted_tags)}\n",
    "    \n",
    "    return word_to_ix, tag_to_ix\n",
    "\n",
    "word_to_ix, tag_to_ix = build_vocab(train_sentences, train_tags)\n",
    "\n",
    "print(f\"\\nVocabulary sizes:\")\n",
    "print(f\"  Word vocabulary: {len(word_to_ix)} words\")\n",
    "print(f\"  Tag vocabulary: {len(tag_to_ix)} tags\")\n",
    "\n",
    "# Create reverse mapping for tags\n",
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "print(f\"\\nTag to index mapping (sorted):\")\n",
    "for tag, idx in sorted(tag_to_ix.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {idx}: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33668518",
   "metadata": {},
   "source": [
    "## Task 2: Create PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3bb615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created:\n",
      "  Train: 14041 sentences\n",
      "  Validation: 3250 sentences\n",
      "  Test: 3453 sentences\n"
     ]
    }
   ],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, word_to_ix, tag_to_ix):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert words to indices\n",
    "        sentence_indices = torch.tensor([\n",
    "            self.word_to_ix.get(word, self.word_to_ix[\"<UNK>\"])\n",
    "            for word in self.sentences[idx]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        # Convert tags to indices\n",
    "        tag_indices = torch.tensor([\n",
    "            self.tag_to_ix[tag]\n",
    "            for tag in self.tags[idx]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        return sentence_indices, tag_indices\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NERDataset(train_sentences, train_tags, word_to_ix, tag_to_ix)\n",
    "val_dataset = NERDataset(val_sentences, val_tags, word_to_ix, tag_to_ix)\n",
    "test_dataset = NERDataset(test_sentences, test_tags, word_to_ix, tag_to_ix)\n",
    "\n",
    "print(f\"Datasets created:\")\n",
    "print(f\"  Train: {len(train_dataset)} sentences\")\n",
    "print(f\"  Validation: {len(val_dataset)} sentences\")\n",
    "print(f\"  Test: {len(test_dataset)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbcc65e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataLoaders created with batch size 32\n",
      "  Train batches: 439\n",
      "  Validation batches: 102\n",
      "  Test batches: 108\n",
      "\n",
      "Sample batch shapes:\n",
      "  Sentences: torch.Size([32, 43])\n",
      "  Tags: torch.Size([32, 43])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to pad sequences in a batch\"\"\"\n",
    "    sentences, tags = zip(*batch)\n",
    "    \n",
    "    # Pad sentences with <PAD> token index (0)\n",
    "    sentences_padded = nn.utils.rnn.pad_sequence(\n",
    "        sentences, \n",
    "        batch_first=True, \n",
    "        padding_value=word_to_ix[\"<PAD>\"]\n",
    "    )\n",
    "    \n",
    "    # Pad tags with -1 for loss calculation (ignore_index)\n",
    "    tags_padded = nn.utils.rnn.pad_sequence(\n",
    "        tags, \n",
    "        batch_first=True, \n",
    "        padding_value=-1\n",
    "    )\n",
    "    \n",
    "    return sentences_padded, tags_padded\n",
    "\n",
    "# Create DataLoaders with fixed seed for reproducibility\n",
    "BATCH_SIZE = 32\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(42)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch size {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test the DataLoader\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  Sentences: {sample_batch[0].shape}\")\n",
    "print(f\"  Tags: {sample_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b972cf",
   "metadata": {},
   "source": [
    "## Task 3: Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab82396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "SimpleRNNForNER(\n",
      "  (embedding): Embedding(23625, 100, padding_idx=0)\n",
      "  (rnn): RNN(100, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n",
      "\n",
      "Model Parameters:\n",
      "  Total parameters: 2,393,101\n",
      "  Trainable parameters: 2,393,101\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNNForNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, padding_idx=0):\n",
    "        super(SimpleRNNForNER, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Linear layer for classification\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding: (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # RNN: (batch_size, seq_len, hidden_dim)\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        \n",
    "        # Linear: (batch_size, seq_len, output_size)\n",
    "        output = self.fc(rnn_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Model hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "OUTPUT_SIZE = len(tag_to_ix)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleRNNForNER(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    padding_idx=word_to_ix[\"<PAD>\"]\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df3460",
   "metadata": {},
   "source": [
    "## Task 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eecb162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Learning rate: 0.001\n",
      "  Epochs: 5\n",
      "  Optimizer: Adam\n",
      "  Loss function: CrossEntropyLoss (ignore_index=-1)\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Use ignore_index=-1 to ignore padding tokens in loss calculation\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Loss function: CrossEntropyLoss (ignore_index=-1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf22cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Starting training...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for sentences, tags in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Move to device\n",
    "        sentences = sentences.to(device)\n",
    "        tags = tags.to(device)\n",
    "        \n",
    "        # 1. Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        outputs = model(sentences)\n",
    "        \n",
    "        # 3. Compute loss\n",
    "        # Reshape for CrossEntropyLoss: (batch_size * seq_len, num_classes)\n",
    "        outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
    "        tags_flat = tags.view(-1)\n",
    "        loss = criterion(outputs_flat, tags_flat)\n",
    "        \n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11af698",
   "metadata": {},
   "source": [
    "## Task 5: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c260396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 439/439 [00:07<00:00, 57.53it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0260\n",
      "Validation Accuracy: 93.36%\n",
      "New best model saved!\n",
      "\n",
      "Epoch 2/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 439/439 [00:07<00:00, 57.11it/s]\n",
      "Training: 100%|██████████| 439/439 [00:07<00:00, 57.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0203\n",
      "Validation Accuracy: 93.84%\n",
      "New best model saved!\n",
      "\n",
      "Epoch 3/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 439/439 [00:08<00:00, 54.51it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0156\n",
      "Validation Accuracy: 93.03%\n",
      "\n",
      "Epoch 4/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 439/439 [00:08<00:00, 54.59it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0128\n",
      "Validation Accuracy: 93.56%\n",
      "\n",
      "Epoch 5/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 439/439 [00:08<00:00, 54.12it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0105\n",
      "Validation Accuracy: 93.41%\n",
      "\n",
      "==================================================\n",
      "Training completed!\n",
      "Best validation accuracy: 93.84%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in dataloader:\n",
    "            # Move to device\n",
    "            sentences = sentences.to(device)\n",
    "            tags = tags.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sentences)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            \n",
    "            # Create mask for non-padding tokens\n",
    "            mask = tags != -1\n",
    "            \n",
    "            # Calculate accuracy only on non-padding tokens\n",
    "            correct += ((predictions == tags) & mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "            \n",
    "            # Store predictions and labels for detailed analysis\n",
    "            all_predictions.extend(predictions[mask].cpu().numpy())\n",
    "            all_labels.extend(tags[mask].cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, all_predictions, all_labels\n",
    "\n",
    "# Train the model\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_accuracy, _, _ = evaluate(model, val_loader, device)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_ner_model.pt')\n",
    "        print(f\"New best model saved!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_accuracy*100:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca588284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Performance:\n",
      "  Accuracy: 90.87%\n"
     ]
    }
   ],
   "source": [
    "# Load best model and evaluate on test set\n",
    "model.load_state_dict(torch.load('best_ner_model.pt'))\n",
    "test_accuracy, test_predictions, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12ff3c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-Tag Accuracy on Test Set:\n",
      "Tag             Count   Accuracy\n",
      "--------------------------------\n",
      "B-LOC            1668     69.96%\n",
      "B-MISC            702     57.83%\n",
      "B-ORG            1661     52.50%\n",
      "B-PER            1617     74.15%\n",
      "I-LOC             257     57.59%\n",
      "I-MISC            216     51.85%\n",
      "I-ORG             835     60.60%\n",
      "I-PER            1156     83.82%\n",
      "O               38323     95.51%\n"
     ]
    }
   ],
   "source": [
    "# Detailed per-tag analysis\n",
    "def analyze_per_tag(predictions, labels, ix_to_tag):\n",
    "    \"\"\"Analyze performance per tag\"\"\"\n",
    "    tag_correct = defaultdict(int)\n",
    "    tag_total = defaultdict(int)\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        tag_name = ix_to_tag[label]\n",
    "        tag_total[tag_name] += 1\n",
    "        if pred == label:\n",
    "            tag_correct[tag_name] += 1\n",
    "    \n",
    "    # Calculate accuracy per tag\n",
    "    tag_accuracies = {}\n",
    "    for tag in tag_total:\n",
    "        tag_accuracies[tag] = tag_correct[tag] / tag_total[tag] if tag_total[tag] > 0 else 0\n",
    "    \n",
    "    return tag_accuracies, tag_total\n",
    "\n",
    "# Analyze test set performance\n",
    "tag_accuracies, tag_counts = analyze_per_tag(test_predictions, test_labels, ix_to_tag)\n",
    "\n",
    "print(\"\\nPer-Tag Accuracy on Test Set:\")\n",
    "print(f\"{'Tag':<12} {'Count':>8} {'Accuracy':>10}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Sort by tag name for better readability\n",
    "for tag in sorted(tag_accuracies.keys()):\n",
    "    acc = tag_accuracies[tag]\n",
    "    count = tag_counts[tag]\n",
    "    print(f\"{tag:<12} {count:>8} {acc*100:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d8d1407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating entity-level metrics...\n",
      "\n",
      "Entity-Level Performance (Test Set):\n",
      "Entity    Precision     Recall   F1-Score    Support\n",
      "----------------------------------------------------\n",
      "PER          43.21%     67.53%     52.70%       1617\n",
      "LOC          81.50%     66.55%     73.27%       1668\n",
      "ORG          62.08%     48.40%     54.40%       1661\n",
      "MISC         63.65%     52.14%     57.32%        702\n",
      "----------------------------------------------------\n",
      "Overall F1-Score: 59.85%\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall, F1 for entity types (not token-level)\n",
    "def get_entities(tags):\n",
    "    \"\"\"Extract entities from BIO tags\"\"\"\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {'type': tag[2:], 'start': i, 'end': i}\n",
    "        elif tag.startswith('I-'):\n",
    "            if current_entity and current_entity['type'] == tag[2:]:\n",
    "                current_entity['end'] = i\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                current_entity = None\n",
    "        else:  # O tag\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def calculate_entity_f1(pred_tags_list, true_tags_list):\n",
    "    \"\"\"Calculate entity-level precision, recall, F1\"\"\"\n",
    "    entity_types = ['PER', 'LOC', 'ORG', 'MISC']\n",
    "    results = {}\n",
    "    \n",
    "    for entity_type in entity_types:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        \n",
    "        for pred_tags, true_tags in zip(pred_tags_list, true_tags_list):\n",
    "            pred_entities = [e for e in get_entities(pred_tags) if e['type'] == entity_type]\n",
    "            true_entities = [e for e in get_entities(true_tags) if e['type'] == entity_type]\n",
    "            \n",
    "            # Convert to sets of (start, end, type) for comparison\n",
    "            pred_set = {(e['start'], e['end'], e['type']) for e in pred_entities}\n",
    "            true_set = {(e['start'], e['end'], e['type']) for e in true_entities}\n",
    "            \n",
    "            tp += len(pred_set & true_set)\n",
    "            fp += len(pred_set - true_set)\n",
    "            fn += len(true_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results[entity_type] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'support': tp + fn\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Reconstruct full tag sequences for test set\n",
    "print(\"\\nCalculating entity-level metrics...\")\n",
    "model.eval()\n",
    "pred_sequences = []\n",
    "true_sequences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sentences, tags in test_loader:\n",
    "        sentences = sentences.to(device)\n",
    "        outputs = model(sentences)\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "        \n",
    "        for pred_seq, true_seq in zip(predictions.cpu().numpy(), tags.cpu().numpy()):\n",
    "            # Remove padding\n",
    "            valid_idx = true_seq != -1\n",
    "            pred_tags = [ix_to_tag[p] for p in pred_seq[valid_idx]]\n",
    "            true_tags = [ix_to_tag[t] for t in true_seq[valid_idx]]\n",
    "            pred_sequences.append(pred_tags)\n",
    "            true_sequences.append(true_tags)\n",
    "\n",
    "# Calculate entity-level metrics\n",
    "entity_results = calculate_entity_f1(pred_sequences, true_sequences)\n",
    "\n",
    "print(\"\\nEntity-Level Performance (Test Set):\")\n",
    "print(f\"{'Entity':<8} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for entity_type in ['PER', 'LOC', 'ORG', 'MISC']:\n",
    "    metrics = entity_results[entity_type]\n",
    "    print(f\"{entity_type:<8} {metrics['precision']*100:>9.2f}% {metrics['recall']*100:>9.2f}% \"\n",
    "          f\"{metrics['f1']*100:>9.2f}% {metrics['support']:>10}\")\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_support = sum(r['support'] for r in entity_results.values())\n",
    "overall_f1 = sum(r['f1'] * r['support'] for r in entity_results.values()) / overall_support if overall_support > 0 else 0\n",
    "\n",
    "print(\"-\" * 52)\n",
    "print(f\"Overall F1-Score: {overall_f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "138f06cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Predictions on Custom Sentences:\n",
      "======================================================================\n",
      "\n",
      "Sentence: VNU University is located in Hanoi\n",
      "Predictions:\n",
      "  VNU                  -> B-ORG\n",
      "  University           -> I-ORG\n",
      "  Hanoi                -> B-PER\n",
      "\n",
      "Sentence: Nguyen Huu Thang works at FPT Corporation in Ho Chi Minh City\n",
      "Predictions:\n",
      "  Nguyen               -> B-PER\n",
      "  Huu                  -> I-PER\n",
      "  Thang                -> I-PER\n",
      "  Corporation          -> I-ORG\n",
      "  Chi                  -> I-PER\n",
      "  City                 -> I-ORG\n",
      "\n",
      "Sentence: The Mekong Delta is in southern Vietnam\n",
      "Predictions:\n",
      "  Mekong               -> B-ORG\n",
      "  Delta                -> I-ORG\n",
      "  Vietnam              -> B-MISC\n",
      "\n",
      "Sentence: Apple Inc. was founded by Steve Jobs in California\n",
      "Predictions:\n",
      "  Apple                -> B-ORG\n",
      "  Inc.                 -> I-ORG\n",
      "  Steve                -> B-PER\n",
      "  Jobs                 -> I-PER\n",
      "  California           -> B-LOC\n",
      "\n",
      "Sentence: Barack Obama visited Paris last year\n",
      "Predictions:\n",
      "  Barack               -> B-PER\n",
      "  Obama                -> I-PER\n",
      "  Paris                -> B-ORG\n",
      "\n",
      "Sentence: Microsoft headquarters is in Redmond Washington\n",
      "Predictions:\n",
      "  Microsoft            -> B-ORG\n",
      "  Redmond              -> B-LOC\n",
      "  Washington           -> B-LOC\n",
      "\n",
      "Sentence: The Amazon river flows through Brazil\n",
      "Predictions:\n",
      "  Amazon               -> B-ORG\n",
      "  Brazil               -> B-LOC\n",
      "\n",
      "Sentence: Google CEO Sundar Pichai announced new products\n",
      "Predictions:\n",
      "  Google               -> B-PER\n",
      "\n",
      "Sentence: The United Nations meeting was held in New York\n",
      "Predictions:\n",
      "  United               -> B-LOC\n",
      "  Nations              -> I-ORG\n",
      "  New                  -> B-LOC\n",
      "  York                 -> I-LOC\n",
      "\n",
      "Sentence: President Joe Biden met with Prime Minister of Japan in Tokyo\n",
      "Predictions:\n",
      "  Joe                  -> B-PER\n",
      "  Biden                -> I-PER\n",
      "  Japan                -> B-LOC\n",
      "  Tokyo                -> B-LOC\n",
      "\n",
      "Sentence: Tesla and SpaceX are both led by Elon Musk\n",
      "Predictions:\n",
      "  Tesla                -> B-PER\n",
      "  SpaceX               -> B-PER\n",
      "  Elon                 -> B-PER\n",
      "  Musk                 -> I-PER\n",
      "\n",
      "Sentence: The World Health Organization issued guidelines for COVID-19\n",
      "Predictions:\n",
      "  World                -> B-MISC\n",
      "  Health               -> I-ORG\n",
      "  Organization         -> I-ORG\n",
      "\n",
      "Sentence: The Wall Street Journal reported on European markets\n",
      "Predictions:\n",
      "  Wall                 -> B-LOC\n",
      "  Street               -> I-ORG\n",
      "  Journal              -> I-ORG\n",
      "  European             -> B-MISC\n",
      "\n",
      "Sentence: Harvard University professor John Smith published research\n",
      "Predictions:\n",
      "  Harvard              -> B-PER\n",
      "  University           -> I-PER\n",
      "  John                 -> B-PER\n",
      "  Smith                -> I-PER\n",
      "\n",
      "Sentence: Samsung and Sony compete in Asian markets\n",
      "Predictions:\n",
      "  Samsung              -> B-ORG\n",
      "  Sony                 -> B-PER\n",
      "  Asian                -> B-LOC\n"
     ]
    }
   ],
   "source": [
    "# Prediction function for new sentences\n",
    "def predict_sentence(sentence_text, model, word_to_ix, ix_to_tag, device):\n",
    "    \"\"\"Predict NER tags for a new sentence\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize (simple whitespace split)\n",
    "    tokens = sentence_text.split()\n",
    "    \n",
    "    # Convert to indices\n",
    "    token_indices = torch.tensor([\n",
    "        word_to_ix.get(token, word_to_ix[\"<UNK>\"]) for token in tokens\n",
    "    ], dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_indices)\n",
    "        predictions = torch.argmax(outputs, dim=-1).squeeze(0)\n",
    "    \n",
    "    # Convert predictions to tags\n",
    "    predicted_tags = [ix_to_tag[pred.item()] for pred in predictions]\n",
    "    \n",
    "    return list(zip(tokens, predicted_tags))\n",
    "\n",
    "# Test prediction function with diverse real-world examples\n",
    "print(\"\\nExample Predictions on Custom Sentences:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_sentences_custom = [\n",
    "    # Vietnamese entities\n",
    "    \"VNU University is located in Hanoi\",\n",
    "    \"Nguyen Huu Thang works at FPT Corporation in Ho Chi Minh City\",\n",
    "    \"The Mekong Delta is in southern Vietnam\",\n",
    "    \n",
    "    # International entities\n",
    "    \"Apple Inc. was founded by Steve Jobs in California\",\n",
    "    \"Barack Obama visited Paris last year\",\n",
    "    \"Microsoft headquarters is in Redmond Washington\",\n",
    "    \n",
    "    # Mixed entities\n",
    "    \"The Amazon river flows through Brazil\",\n",
    "    \"Google CEO Sundar Pichai announced new products\",\n",
    "    \"The United Nations meeting was held in New York\",\n",
    "    \n",
    "    # Complex entities\n",
    "    \"President Joe Biden met with Prime Minister of Japan in Tokyo\",\n",
    "    \"Tesla and SpaceX are both led by Elon Musk\",\n",
    "    \"The World Health Organization issued guidelines for COVID-19\",\n",
    "    \n",
    "    # Edge cases\n",
    "    \"The Wall Street Journal reported on European markets\",\n",
    "    \"Harvard University professor John Smith published research\",\n",
    "    \"Samsung and Sony compete in Asian markets\"\n",
    "]\n",
    "\n",
    "for sent in test_sentences_custom:\n",
    "    predictions = predict_sentence(sent, model, word_to_ix, ix_to_tag, device)\n",
    "    \n",
    "    # Only show sentence if there are predicted entities\n",
    "    has_entity = any(tag != 'O' for _, tag in predictions)\n",
    "    if has_entity:\n",
    "        print(f\"\\nSentence: {sent}\")\n",
    "        print(\"Predictions:\")\n",
    "        for token, tag in predictions:\n",
    "            if tag != 'O':\n",
    "                print(f\"  {token:20s} -> {tag}\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Sentence: {sent}\")\n",
    "        print(\"  No entities detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71afc17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real Examples from Test Set:\n",
      "======================================================================\n",
      "\n",
      "Sentence: ( 52.76 / 53.18 )\n",
      "Token                True         Predicted   \n",
      "--------------------------------------------\n",
      "52.76                O            B-PER        DIFF\n",
      "53.18                O            B-PER        DIFF\n",
      "\n",
      "Sentence: WESTERN CONFERENCE\n",
      "Token                True         Predicted   \n",
      "--------------------------------------------\n",
      "WESTERN              O            B-MISC       DIFF\n",
      "CONFERENCE           O            B-PER        DIFF\n",
      "\n",
      "Sentence: Wasim Akram b Harris 4\n",
      "Token                True         Predicted   \n",
      "--------------------------------------------\n",
      "Wasim                B-PER        B-PER        OK\n",
      "Akram                I-PER        I-PER        OK\n",
      "Harris               B-PER        B-PER        OK\n",
      "\n",
      "Sentence: Mansfield 21 5 9 7 21 22 24\n",
      "Token                True         Predicted   \n",
      "--------------------------------------------\n",
      "Mansfield            B-ORG        B-ORG        OK\n",
      "\n",
      "Sentence: -- New York Commodities Desk , 212-859-1640\n",
      "Token                True         Predicted   \n",
      "--------------------------------------------\n",
      "New                  B-ORG        B-LOC        DIFF\n",
      "York                 I-ORG        I-LOC        DIFF\n",
      "Commodities          I-ORG        O            DIFF\n",
      "212-859-1640         O            B-PER        DIFF\n"
     ]
    }
   ],
   "source": [
    "# Show some real examples from test set\n",
    "print(\"\\nReal Examples from Test Set:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show 5 examples\n",
    "import random\n",
    "random.seed(42)\n",
    "sample_indices = random.sample(range(len(test_sentences)), 5)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    tokens = test_sentences[idx]\n",
    "    true_tags = test_tags[idx]\n",
    "    \n",
    "    predictions = predict_sentence(' '.join(tokens), model, word_to_ix, ix_to_tag, device)\n",
    "    pred_tags = [tag for _, tag in predictions]\n",
    "    \n",
    "    print(f\"\\nSentence: {' '.join(tokens)}\")\n",
    "    print(f\"{'Token':<20s} {'True':<12s} {'Predicted':<12s}\")\n",
    "    print(\"-\" * 44)\n",
    "    \n",
    "    for token, true_tag, pred_tag in zip(tokens, true_tags, pred_tags):\n",
    "        if true_tag != 'O' or pred_tag != 'O':\n",
    "            match = \"OK\" if true_tag == pred_tag else \"DIFF\"\n",
    "            print(f\"{token:<20s} {true_tag:<12s} {pred_tag:<12s} {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01473813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Model Architecture: SimpleRNN\n",
      "  Embedding dimension: 100\n",
      "  Hidden dimension: 128\n",
      "  Total parameters: 2,393,101\n",
      "\n",
      "Dataset:\n",
      "  Training sentences: 14041\n",
      "  Validation sentences: 3250\n",
      "  Test sentences: 3453\n",
      "\n",
      "Training:\n",
      "  Epochs: 5\n",
      "  Batch size: 32\n",
      "  Learning rate: 0.001\n",
      "\n",
      "Performance:\n",
      "  Best validation accuracy: 93.11%\n",
      "  Test accuracy: 90.41%\n",
      "  Overall F1-score: 59.85%\n",
      "\n",
      "Per-Entity F1-Scores:\n",
      "  PER: 52.70%\n",
      "  LOC: 73.27%\n",
      "  ORG: 54.40%\n",
      "  MISC: 57.32%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel Architecture: SimpleRNN\")\n",
    "print(f\"  Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  Hidden dimension: {HIDDEN_DIM}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training sentences: {len(train_sentences)}\")\n",
    "print(f\"  Validation sentences: {len(val_sentences)}\")\n",
    "print(f\"  Test sentences: {len(test_sentences)}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Best validation accuracy: {best_val_accuracy*100:.2f}%\")\n",
    "print(f\"  Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"  Overall F1-score: {overall_f1*100:.2f}%\")\n",
    "print(\"\\nPer-Entity F1-Scores:\")\n",
    "for entity_type in ['PER', 'LOC', 'ORG', 'MISC']:\n",
    "    f1 = entity_results[entity_type]['f1']\n",
    "    print(f\"  {entity_type}: {f1*100:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
